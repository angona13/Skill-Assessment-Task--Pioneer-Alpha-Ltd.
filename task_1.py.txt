# -*- coding: utf-8 -*-
"""Task 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n4AXnbAbUHiu8s02lq4W-kjQkpAqm07w
"""

# %% Named Entity Recognition is a standard NLP task that can identify entities discussed in a text document. 
# %% Named Entity Recognition is implemented by the 'pipeline' component 'ner'. 
# Import and load the spacy model
import spacy
nlp=spacy.load("en_core_web_sm") 
nlp.pipe_names

#Getting the ner component
ner=nlp.get_pipe('ner')

# %%spaCy accepts training data as list of tuples. Each tuple should contain the text and a dictionary. 
# %%The dictionary should hold the start and end indices of the named enity in the text, and the category or label of the named entity.
# Importing requirements
# # Training completely new entity type in spaCy

# New label to add
LABEL = "FOOD"

# Training examples in the required format
TRAIN_DATA =[ ("Pizza is a common fast food.", {"entities": [(0, 5, "FOOD")]}),
              ("Pasta is an italian recipe", {"entities": [(0, 5, "FOOD")]}),
              ("Anita likes chicken-fry very much", {"entities": [(12,23, "FOOD")]}),
              ("China's noodles are very famous", {"entities": [(8,14, "FOOD")]}),
              ("Shrimps are famous in China too", {"entities": [(0,7, "FOOD")]}),
              ("Lasagna is another classic of Italy", {"entities": [(0,7, "FOOD")]}),
              ("Sushi is extemely famous and expensive Japanese dish", {"entities": [(0,5, "FOOD")]}),
              ("Unagi is a famous seafood of Japan", {"entities": [(0,5, "FOOD")]}),
              ("Tempura , Soba are other famous dishes of Japan", {"entities": [(0,7, "FOOD")]}),
              ("Udon is a healthy type of noodles", {"entities": [(0,4, "FOOD")]}),
              ("Korean used to eat Rameyon as a healthy type of noodles", {"entities": [(19,26, "FOOD")]}),
              ("Chocolate soufflé is extremely famous french cuisine", {"entities": [(0,17, "FOOD")]}),
              ("Flamiche is french pastry", {"entities": [(0,8, "FOOD")]}),
              ("Burgers are the most commonly consumed fastfood", {"entities": [(0,7, "FOOD")]}),
              ("Burgers are the most commonly consumed fastfood", {"entities": [(0,7, "FOOD")]}),
              ("Frenchfries are considered too oily", {"entities": [(0,11, "FOOD")]}),
              ("Fish is most common in Asia", {"entities": [(0,4, "FOOD")]})
           ]

# the label “FOOD” label is not known to the model. So, the first task will be to add the label to ner through add_label() method. 
# then we can use resume_training() function to return an optimizer.
# when training is done the other pipeline components will also get affected . To prevent these, disable_pipes() is used in this method to disable all other pipes.
# Add the new label to ner
ner.add_label(LABEL)

# Resume training
optimizer = nlp.resume_training()
move_names = list(ner.move_names)

# List of pipes you want to train
pipe_exceptions = ["ner", "trf_wordpiecer", "trf_tok2vec"]

# List of pipes which should remain unaffected in training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]

from spacy.util import minibatch, compounding
import random
from spacy.training.example import Example

# Begin training by disabling other pipeline components
with nlp.disable_pipes(*other_pipes) :

 # sizes = compounding(1.0, 4.0, 1.001)
  # Training for 30 iterations     
  #for itn in range(30):
    # shuffle examples before training
   # random.shuffle(TRAIN_DATA)
    # batch up the examples using spaCy's minibatch
    #batches = minibatch(TRAIN_DATA, size=sizes)
    # ictionary to store losses
 #   losses = {}
    #for batch in batches:
     #   texts, annotations = zip(*batch)
      # Calling update() over the iteration

        #nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)
        #print("Losses", losses)

       
  for batch in spacy.util.minibatch(TRAIN_DATA, size= 4):
        losses = {}  # A dictionary to hold the losses against each pipeline component.
        for text, annotations in batch:
        # create Example
            doc = nlp.make_doc(text)
            example = Example.from_dict(doc, annotations)
        # Update the model
            nlp.update([example], sgd=optimizer, losses=losses, drop=0.35) # 'drop' = represents the dropout rate.
        print("Losses", losses)

# Testing the NER

from spacy import displacy

#test_text = "I ate Sushi yesterday. Maggi is a common fast food."
test_text = "I love Sushi. But here Burgers is the most popular fast food. "
doc = nlp(test_text)
print("Entities in '%s'" % test_text)
for ent in doc.ents:
  print("Detected Foods are: ", ent)

#displacy.render(TRAIN_DATA, style="ent",jupyter=True)

# % Conclusion: The observed the above output ensures that the model has correctly identified the FOOD items. 
# Also, it is noticeable that ” Maggi ” had not passed  as a training example to the model. 
# Still, based on the similarity of context, the model has identified “Maggi” also asFOOD. 
# This is an important requirement that our model should not just memorize the training examples.